import os
import re
import streamlit as st
from dotenv import load_dotenv

from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

import pandas as pd
import time
from typing import List, Dict, Tuple

# RAGAS imports
from ragas import evaluate
from ragas.metrics import (
    answer_relevancy,
    faithfulness,
    context_recall,
    context_precision
)
from datasets import Dataset

# ---------------------------
# Config
# ---------------------------
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    st.error("âš ï¸ LÃ¼tfen .env dosyasÄ±na OPENAI_API_KEY ekleyin.")
    st.stop()

st.set_page_config(page_title="Ã‡ocuk Okul EÄŸitimi Chatbot", layout="wide")
st.title("ğŸ“š Ã‡ocuk EÄŸitimi Ä°statistikleri Chatbot")
st.caption("OpenAI GPT + RAGAS ile performans deÄŸerlendirmeli versiyon")

DATA_FOLDER = "data"
PERSIST_DIR = "./chroma_db"

# LLM ve Embeddings
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.1,
    max_tokens=1000,
    api_key=api_key
)

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    api_key=api_key
)

# Sidebar ayarlar
with st.sidebar:
    st.header("âš™ï¸ Ayarlar")
    
    debug_mode = st.checkbox("ğŸ”§ Debug Modu", value=False, help="Hata mesajlarÄ±nÄ± detaylÄ± gÃ¶ster")
    
    st.divider()
    
    max_docs_per_file = st.slider(
        "Dosya baÅŸÄ±na maksimum dokÃ¼man",
        min_value=10,
        max_value=100,
        value=30,
        step=10,
        help="Daha az dokÃ¼man = daha az embedding kota kullanÄ±mÄ±"
    )
    
    batch_size = st.slider(
        "Batch boyutu (embedding)",
        min_value=5,
        max_value=50,
        value=10,
        step=5,
        help="KÃ¼Ã§Ã¼k batch = daha yavaÅŸ ama gÃ¼venli"
    )
    
    sleep_time = st.slider(
        "Batch arasÄ± bekleme (saniye)",
        min_value=0,
        max_value=10,
        value=1,
        step=1,
        help="Rate limit iÃ§in bekleme sÃ¼resi"
    )
    
    retriever_k = st.slider(
        "Retriever K (dÃ¶kÃ¼man sayÄ±sÄ±)",
        min_value=2,
        max_value=10,
        value=4,
        step=1,
        help="Her sorguda kaÃ§ dokÃ¼man getirilecek"
    )


# ---------------------------
# Helpers
# ---------------------------
def parse_tuik_pipe_rows(file_path: str, encoding="utf-8") -> List[Dict]:
    """TÃœÄ°K pipe-delimited formatÄ±nÄ± parse eder"""
    rows = []
    current_metric = None
    current_breakdown = None
    current_geo = None

    def is_year(s: str) -> bool:
        if not s:
            return False
        s = s.strip()
        return s.isdigit() and 1900 <= int(s) <= 2100

    def parse_value(s: str):
        if not s:
            return None
        s = str(s).strip().replace(",", ".")
        s = re.sub(r'[^\d.\-]', '', s)
        try:
            return float(s)
        except:
            return None

    try:
        with open(file_path, "r", encoding=encoding, errors="replace") as f:
            for line_num, raw in enumerate(f, 1):
                line = raw.strip()
                if not line:
                    continue
                    
                if "SÃ¼tunlar" in line or line.startswith("SatÄ±rlar"):
                    continue

                parts = [p.strip() for p in line.split("|")]
                
                while parts and parts[0] == "":
                    parts.pop(0)
                while parts and parts[-1] == "":
                    parts.pop()

                if not parts:
                    continue

                if len(parts) == 1 and re.match(r".+-[A-Z]{2}$", parts[0]):
                    current_geo = parts[0]
                    continue

                if len(parts) >= 4 and is_year(parts[-2]):
                    metric, breakdown, year, value = parts[-4], parts[-3], parts[-2], parts[-1]
                    if metric:
                        current_metric = metric
                    if breakdown:
                        current_breakdown = breakdown

                    parsed_value = parse_value(value)
                    if parsed_value is not None:
                        rows.append({
                            "metric": current_metric,
                            "breakdown": current_breakdown,
                            "year": int(year),
                            "value": parsed_value,
                            "geo": current_geo or "TÃ¼rkiye-TR",
                            "source_line": line_num
                        })
                    continue

                if len(parts) == 3 and is_year(parts[1]):
                    breakdown, year, value = parts
                    if breakdown:
                        current_breakdown = breakdown
                    
                    parsed_value = parse_value(value)
                    if parsed_value is not None:
                        rows.append({
                            "metric": current_metric,
                            "breakdown": current_breakdown,
                            "year": int(year),
                            "value": parsed_value,
                            "geo": current_geo or "TÃ¼rkiye-TR",
                            "source_line": line_num
                        })
                    continue

                if len(parts) == 2 and is_year(parts[0]):
                    year, value = parts
                    parsed_value = parse_value(value)
                    if parsed_value is not None:
                        rows.append({
                            "metric": current_metric,
                            "breakdown": current_breakdown,
                            "year": int(year),
                            "value": parsed_value,
                            "geo": current_geo or "TÃ¼rkiye-TR",
                            "source_line": line_num
                        })
                    continue

                if len(parts) == 2:
                    m, b = parts
                    if m:
                        current_metric = m
                    if b:
                        current_breakdown = b

        valid_rows = [r for r in rows if r["year"] and r["value"] is not None]
        return valid_rows
    
    except Exception as e:
        st.warning(f"âš ï¸ {file_path} dosyasÄ± okunurken hata: {str(e)}")
        return []


def build_docs_for_one_csv(file_path: str, max_docs=30) -> List[Document]:
    """KOTA OPTÄ°MÄ°ZASYONU: Minimum dokÃ¼man ile maksimum bilgi"""
    rows = parse_tuik_pipe_rows(file_path)
    if not rows:
        return []

    dataset_name = os.path.basename(file_path).replace(".csv", "")
    df = pd.DataFrame(rows)
    
    docs = []
    
    years = sorted(df["year"].unique())
    metrics = df["metric"].unique()
    
    summary = (
        f"ğŸ“Š {dataset_name}\n"
        f"YÄ±llar: {years[0]}-{years[-1]}\n"
        f"Metrikler: {', '.join(str(m) for m in metrics[:5])}\n"
        f"Toplam veri: {len(df)} kayÄ±t\n"
        f"Ortalama deÄŸer: {df['value'].mean():.2f}"
    )
    
    docs.append(Document(
        page_content=summary,
        metadata={"type": "dataset_summary", "dataset": dataset_name}
    ))
    
    yearly_summaries = []
    for year in years[-5:]:
        year_data = df[df["year"] == year]
        yearly_summaries.append(
            f"{year}: Ort={year_data['value'].mean():.1f}, "
            f"Min={year_data['value'].min():.1f}, "
            f"Max={year_data['value'].max():.1f}"
        )
    
    if yearly_summaries:
        docs.append(Document(
            page_content=f"{dataset_name} - YÄ±llÄ±k Ä°statistikler:\n" + "\n".join(yearly_summaries),
            metadata={"type": "yearly_stats", "dataset": dataset_name}
        ))
    
    metric_summaries = []
    for metric in metrics[:5]:
        metric_data = df[df["metric"] == metric]
        metric_summaries.append(
            f"{metric}: {len(metric_data)} kayÄ±t, Ort={metric_data['value'].mean():.1f}"
        )
    
    if metric_summaries:
        docs.append(Document(
            page_content=f"{dataset_name} - Metrikler:\n" + "\n".join(metric_summaries),
            metadata={"type": "metric_stats", "dataset": dataset_name}
        ))
    
    important_rows = []
    recent_years = sorted(df["year"].unique())[-3:]
    important_rows.extend(df[df["year"].isin(recent_years)].to_dict('records'))
    important_rows.extend(df.nlargest(5, 'value').to_dict('records'))
    important_rows.extend(df.nsmallest(5, 'value').to_dict('records'))
    
    seen = set()
    unique_rows = []
    for row in important_rows:
        key = (row['year'], row['metric'], row['breakdown'])
        if key not in seen:
            seen.add(key)
            unique_rows.append(row)
    
    for row in unique_rows[:max_docs-3]:
        geo = row.get('geo', 'TÃ¼rkiye-TR')
        content = (
            f"{geo}, {row['year']}: "
            f"{row['metric']} ({row['breakdown']}) = {row['value']}"
        )
        
        docs.append(Document(
            page_content=content,
            metadata={
                "type": "data_point",
                "dataset": dataset_name,
                "year": row["year"],
                "value": row["value"],
                "metric": row["metric"],
                "breakdown": row["breakdown"]
            }
        ))
    
    return docs


def format_docs(docs):
    """DÃ¶kÃ¼manlarÄ± formatla"""
    if not docs:
        return "Ä°lgili veri bulunamadÄ±."
    return "\n\n".join(d.page_content for d in docs)


@st.cache_resource
def load_all_rows_as_df():
    """TÃ¼m CSV dosyalarÄ±nÄ± DataFrame olarak yÃ¼kler"""
    all_rows = []
    
    if not os.path.exists(DATA_FOLDER):
        st.error(f"âŒ {DATA_FOLDER} klasÃ¶rÃ¼ bulunamadÄ±!")
        return pd.DataFrame()
    
    csv_files = [f for f in os.listdir(DATA_FOLDER) if f.endswith(".csv")]
    
    if not csv_files:
        st.warning(f"âš ï¸ {DATA_FOLDER} klasÃ¶rÃ¼nde CSV dosyasÄ± bulunamadÄ±!")
        return pd.DataFrame()
    
    for fn in csv_files:
        fp = os.path.join(DATA_FOLDER, fn)
        rows = parse_tuik_pipe_rows(fp)
        for r in rows:
            r["source_file"] = fn
        all_rows.extend(rows)

    if not all_rows:
        return pd.DataFrame()

    df = pd.DataFrame(all_rows)
    df["metric"] = df["metric"].fillna("").astype(str)
    df["breakdown"] = df["breakdown"].fillna("").astype(str)
    df["geo"] = df["geo"].fillna("TÃ¼rkiye-TR").astype(str)
    
    return df


def prepare_vector_db(max_docs_per_file, batch_size, sleep_time):
    """Vector DB hazÄ±rla"""
    
    # CSV dosyalarÄ±nÄ± kontrol et
    if not os.path.exists(DATA_FOLDER):
        st.error(f"âŒ '{DATA_FOLDER}' klasÃ¶rÃ¼ bulunamadÄ±!")
        st.info(f"ğŸ’¡ LÃ¼tfen '{DATA_FOLDER}' klasÃ¶rÃ¼ oluÅŸturun ve CSV dosyalarÄ±nÄ±zÄ± iÃ§ine koyun")
        return None
    
    csv_files = [f for f in os.listdir(DATA_FOLDER) if f.endswith(".csv")]
    
    if not csv_files:
        st.error(f"âŒ '{DATA_FOLDER}' klasÃ¶rÃ¼nde CSV dosyasÄ± bulunamadÄ±!")
        st.info("ğŸ’¡ LÃ¼tfen TÃœÄ°K CSV dosyalarÄ±nÄ±zÄ± 'data/' klasÃ¶rÃ¼ne ekleyin")
        return None
    
    # Mevcut DB'yi kontrol et
    if os.path.exists(PERSIST_DIR) and os.listdir(PERSIST_DIR):
        try:
            st.info("â™»ï¸ Mevcut vector DB yÃ¼kleniyor...")
            vectorstore = Chroma(
                persist_directory=PERSIST_DIR,
                embedding_function=embeddings
            )

            # Embedding harcamadan DB dolu mu kontrol et
            try:
                _count = vectorstore._collection.count()
                st.info(f"ğŸ“¦ DB dokÃ¼man sayÄ±sÄ±: {_count}")
                if _count == 0:
                    raise ValueError("DB boÅŸ gÃ¶rÃ¼nÃ¼yor")
            except Exception:
                pass

            st.success("âœ… Mevcut DB baÅŸarÄ±yla yÃ¼klendi")
            return vectorstore

        except Exception as e:
            st.warning(f"âš ï¸ Mevcut DB uyumsuz: {str(e)}")
            st.info("ğŸ”„ Yeni DB oluÅŸturuluyor...")
            import shutil
            try:
                shutil.rmtree(PERSIST_DIR)
            except Exception as rm_err:
                st.warning(f"DB silinirken hata: {rm_err}")

      

    st.warning("ğŸ”„ Vector DB oluÅŸturuluyor...")
    st.info(f"âš™ï¸ Ayarlar: {len(csv_files)} dosya, Dosya baÅŸÄ±na {max_docs_per_file} doc, Batch={batch_size}")
    
    all_docs = []
    
    # DÃ¶kÃ¼manlarÄ± topla
    for fn in csv_files:
        fp = os.path.join(DATA_FOLDER, fn)
        try:
            file_docs = build_docs_for_one_csv(fp, max_docs=max_docs_per_file)
            all_docs.extend(file_docs)
            st.caption(f"âœ“ {fn}: {len(file_docs)} dokÃ¼man")
        except Exception as e:
            st.warning(f"âš ï¸ {fn} iÅŸlenirken hata: {str(e)}")
            continue
    
    if not all_docs:
        st.error("âŒ HiÃ§ dokÃ¼man Ã¼retilemedi!")
        st.info("ğŸ’¡ CSV dosyalarÄ±nÄ±zÄ±n formatÄ±nÄ± kontrol edin")
        return None
    
    st.info(f"ğŸ“¦ Toplam {len(all_docs)} dokÃ¼man, {(len(all_docs)-1)//batch_size + 1} batch'te iÅŸlenecek")
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    vectorstore = None
    successful_batches = 0
    
    for i in range(0, len(all_docs), batch_size):
        batch = all_docs[i:i+batch_size]
        batch_num = i//batch_size + 1
        total_batches = (len(all_docs)-1)//batch_size + 1
        
        status_text.text(f"â³ Batch {batch_num}/{total_batches} iÅŸleniyor...")
        
        try:
            if vectorstore is None:
                vectorstore = Chroma.from_documents(
                    documents=batch,
                    embedding=embeddings,
                    persist_directory=PERSIST_DIR
                )
            else:
                vectorstore.add_documents(batch)
            
            successful_batches += 1
            progress_bar.progress(min((i + batch_size) / len(all_docs), 1.0))
            
            if i + batch_size < len(all_docs) and sleep_time > 0:
                status_text.text(f"âœ“ Batch {batch_num} tamamlandÄ±. {sleep_time}s bekleniyor...")
                time.sleep(sleep_time)
        
        except Exception as e:
            st.error(f"âŒ Batch {batch_num} hatasÄ±: {str(e)}")
            
            # Rate limit kontrolÃ¼
            if "rate" in str(e).lower() or "quota" in str(e).lower() or "429" in str(e):
                st.warning("âš ï¸ Rate limit! 30 saniye bekleniyor...")
                time.sleep(30)
            else:
                st.warning("â¸ï¸ 10 saniye bekleniyor...")
                time.sleep(10)
            
            # Tekrar dene
            try:
                if vectorstore is None:
                    vectorstore = Chroma.from_documents(
                        documents=batch,
                        embedding=embeddings,
                        persist_directory=PERSIST_DIR
                    )
                else:
                    vectorstore.add_documents(batch)
                successful_batches += 1
            except Exception as e2:
                st.error(f"âŒ Batch {batch_num} ikinci denemede de baÅŸarÄ±sÄ±z: {str(e2)}")
                # Ä°lk batch baÅŸarÄ±sÄ±z olduysa dur
                if batch_num == 1:
                    progress_bar.empty()
                    status_text.empty()
                    st.error("âŒ Ä°lk batch oluÅŸturulamadÄ±. API key'inizi ve internet baÄŸlantÄ±nÄ±zÄ± kontrol edin.")
                    return None
                continue
    
    progress_bar.empty()
    status_text.empty()
    
    if vectorstore is None:
        st.error("âŒ Vector DB oluÅŸturulamadÄ±!")
        return None
    
    st.success(f"âœ… Vector DB oluÅŸturuldu! ({successful_batches}/{total_batches} batch baÅŸarÄ±lÄ±)")
    
    return vectorstore


def extract_years(query: str) -> List[int]:
    """Sorgudan yÄ±l bilgilerini Ã§Ä±kar"""
    q = query.replace("â€“", "-").replace("â€”", "-")
    years = sorted(set(int(y) for y in re.findall(r"(19\d{2}|20\d{2})", q)))
    
    m = re.search(r"(19\d{2}|20\d{2})\s*-\s*(19\d{2}|20\d{2})", q)
    if m:
        a, b = int(m.group(1)), int(m.group(2))
        if a > b:
            a, b = b, a
        return list(range(a, b + 1))
    return years


def normalize_tr(s: str) -> str:
    s = (s or "").strip()
    s = s.replace("Ä°", "i").replace("I", "Ä±")
    return s.lower()



def df_lookup_answer(df_all: pd.DataFrame, user_query: str, max_rows=50) -> Tuple[str, bool]:
    """Direkt DataFrame lookup - embedding kullanmadan"""
    if df_all is None or df_all.empty:
        return ("", False)

    q = normalize_tr(user_query)
    years = extract_years(q)

    keywords = [
    "eÄŸitim","ortaÃ¶ÄŸretim","ilkokul","ortaokul","anaokulu","okul Ã¶ncesi","kreÅŸ",
    "brÃ¼t","okullaÅŸma","cinsiyet","oran","yÄ±llÄ±k",
    "derslik","Ã¶ÄŸretmen","memnuniyet","devlet okulu","Ã¶zel okul"
]    
    structured = (len(years) > 0) and any(k in q for k in keywords)

    if not structured:
        return ("", False)

    df = df_all.copy()

    if years:
        df = df[df["year"].isin(years)]

    # metrik hedefleme
    if "memnuniyet" in q:
        df = df[df["metric"].str.contains("memnuniyet", case=False, na=False)]

    if "derslik" in q:
        df = df[df["metric"].str.contains("Derslik", case=False, na=False)]

    if "Ã¶ÄŸretmen" in q:
        df = df[df["metric"].str.contains("Ã–ÄŸretmen", case=False, na=False)]

    if "kreÅŸ" in q or "anaokulu" in q or "okul Ã¶ncesi" in q:
        df = df[df["metric"].str.contains("kreÅŸ|anaokul|okul Ã¶nces", case=False, na=False)]

    if "ortaÃ¶ÄŸretim" in q:
        df = df[df["breakdown"].str.contains("OrtaÃ¶ÄŸretim", case=False, na=False)]
    if "ilkokul" in q:
        df = df[df["breakdown"].str.contains("Ä°lkokul", case=False, na=False)]
    if "brÃ¼t" in q or "okullaÅŸma" in q:
        df = df[df["metric"].str.contains("OkullaÅŸma", case=False, na=False)]
    if "cinsiyet" in q:
        df = df[df["metric"].str.contains("Cinsiyet", case=False, na=False)]

    if "devlet" in q or "resmi" in q:
        df = df[df["breakdown"].str.contains("devlet|resmi", case=False, na=False)]

    if "Ã¶zel" in q:
        df = df[df["breakdown"].str.contains("Ã¶zel", case=False, na=False)]


    if df.empty:
        return ("", False)

    df = df.sort_values(["year", "metric", "breakdown"]).head(max_rows)

    lines = []
    for _, r in df.iterrows():
        geo = r.get("geo", "TÃ¼rkiye-TR")
        lines.append(f"{r['year']}: {geo} | {r['metric']} ({r['breakdown']}) = {r['value']}")

    answer = "ğŸ“Š VeritabanÄ±nda bulunan eÅŸleÅŸen kayÄ±tlar:\n\n" + "\n".join(lines)

    if years and (len(years) >= 2):
        found_years = set(df["year"].unique())
        missing = [y for y in years if y not in found_years]
        if missing:
            answer += f"\n\nâš ï¸ Not: Bu filtrelerle ÅŸu yÄ±llar iÃ§in kayÄ±t Ã§Ä±kmadÄ±: {missing}"

    return (answer, True)


# ---------------------------
# RAGAS Evaluation
# ---------------------------

def create_evaluation_dataset() -> List[Dict]:
    """Test iÃ§in Ã¶rnek veri seti oluÅŸtur"""
    return [
        {
            "question": "2020 yÄ±lÄ±nda ilkokul okullaÅŸma oranÄ± nedir?",
            "ground_truth": "2020 yÄ±lÄ± ilkokul okullaÅŸma oranÄ± hakkÄ±nda bilgi"
        },
        {
            "question": "OrtaÃ¶ÄŸretim ve ilkokul Ã¶ÄŸrenci sayÄ±larÄ±nÄ± karÅŸÄ±laÅŸtÄ±r",
            "ground_truth": "OrtaÃ¶ÄŸretim ve ilkokul Ã¶ÄŸrenci sayÄ±larÄ± karÅŸÄ±laÅŸtÄ±rmasÄ±"
        },
        {
            "question": "Son 5 yÄ±lda Ã¶ÄŸretmen sayÄ±sÄ± nasÄ±l deÄŸiÅŸti?",
            "ground_truth": "Son 5 yÄ±lda Ã¶ÄŸretmen sayÄ±sÄ±ndaki deÄŸiÅŸim trendi"
        }
    ]


def evaluate_rag_with_ragas(rag_chain, retriever, test_questions: List[Dict]) -> Dict:
    """RAGAS ile RAG sistemini deÄŸerlendir"""
    
    st.info("ğŸ”„ RAGAS deÄŸerlendirmesi baÅŸlatÄ±lÄ±yor...")
    
    questions = []
    answers = []
    contexts = []
    ground_truths = []
    
    progress_bar = st.progress(0)
    
    for idx, item in enumerate(test_questions):
        question = item["question"]
        ground_truth = item["ground_truth"]
        
        try:
            answer = rag_chain.invoke(question)
            retrieved_docs = retriever.invoke(question)
            context = [doc.page_content for doc in retrieved_docs]
            
            questions.append(question)
            answers.append(answer)
            contexts.append(context)
            ground_truths.append(ground_truth)
            
            progress_bar.progress((idx + 1) / len(test_questions))
            
        except Exception as e:
            st.warning(f"âš ï¸ Soru iÅŸlenirken hata: {question[:50]}... | {str(e)}")
            continue
    
    progress_bar.empty()
    
    if not questions:
        st.error("âŒ HiÃ§ soru iÅŸlenemedi!")
        return {}
    
    eval_dataset = Dataset.from_dict({
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths
    })
    
    st.info("ğŸ“Š RAGAS metrikleri hesaplanÄ±yor...")

    try:
        result = evaluate(
            eval_dataset,
            metrics=[
                answer_relevancy,
                faithfulness,
                context_recall,
                context_precision,
            ]
        )

        # ğŸ”¹ Tabloya Ã§evirmeyi dene (ragas >= 0.1.7)
        try:
            df_metrics = result.to_pandas()
            st.subheader("ğŸ“ˆ RAGAS SonuÃ§larÄ± (Tablo)")
            st.dataframe(df_metrics)
        except Exception:
            st.subheader("ğŸ“ˆ RAGAS Ham SonuÃ§")
            st.write(result)

        # ğŸ”¹ Tekil skorlarÄ± gÃ¼venli ÅŸekilde Ã§Ä±kar
        scores = {}
        for metric in ["answer_relevancy", "faithfulness", "context_recall", "context_precision"]:
            try:
                scores[metric] = float(result[metric])
            except Exception:
                pass

        return scores

    except Exception as e:
        st.error(f"âŒ RAGAS deÄŸerlendirme hatasÄ±: {str(e)}")
        return {}



# ---------------------------
# Main App
# ---------------------------

try:
    vector_store = prepare_vector_db(max_docs_per_file, batch_size, sleep_time)
    
    if vector_store is None:
        st.error("âŒ Vector DB oluÅŸturulamadÄ±!")
        st.info("ğŸ’¡ LÃ¼tfen 'data/' klasÃ¶rÃ¼nde CSV dosyalarÄ±nÄ±zÄ±n olduÄŸundan emin olun")
        st.stop()
    
    st.success("âœ… Vector DB hazÄ±r")
    
except Exception as e:
    st.error(f"âŒ Vector DB hatasÄ±: {str(e)}")
    st.info("ğŸ’¡ OlasÄ± Ã§Ã¶zÃ¼mler:")
    st.info("1. 'data/' klasÃ¶rÃ¼nde CSV dosyalarÄ± var mÄ± kontrol edin")
    st.info("2. OpenAI API key'inizin geÃ§erli olduÄŸunu kontrol edin")
    st.info("3. Sidebar'dan 'Vector DB SÄ±fÄ±rla' butonuna tÄ±klayÄ±n")
    st.info("4. Ä°nternet baÄŸlantÄ±nÄ±zÄ± kontrol edin")
    
    if debug_mode:
        st.exception(e)
    
    st.stop()

retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={
        "k": retriever_k,
        "filter": {"type": "data_point"}   # <-- kritik
    }
)

prompt = ChatPromptTemplate.from_template(
"""Sen Ã§ocuk eÄŸitimi istatistikleri konusunda uzman bir asistansÄ±n. TÃœÄ°K verilerini kullanarak sorularÄ± yanÄ±tlÄ±yorsun.

ğŸ“‹ BaÄŸlam (Retrieved Data):
{context}

â“ Soru: {question}

ğŸ“Œ Kurallar:
1. Sadece verilen baÄŸlamdaki bilgileri kullan
2. BaÄŸlamda bilgi yoksa "Bu konuda veri bulamadÄ±m" de ve alternatif Ã¶ner
3. SayÄ±larÄ± ve istatistikleri net ve doÄŸru belirt
4. KÄ±sa, Ã¶z ve anlaÅŸÄ±lÄ±r yanÄ±t ver
5. Gerekirse karÅŸÄ±laÅŸtÄ±rma yap
6. YÄ±l bilgisini mutlaka belirt

âœ… Cevap:"""
)

context_runnable = retriever | RunnableLambda(format_docs)

rag_chain = (
    {"context": context_runnable, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

df_all = load_all_rows_as_df()

# Sidebar stats & controls
with st.sidebar:
    st.divider()
    st.header("ğŸ“Š Ä°statistikler")
    
    if not df_all.empty:
        st.metric("Toplam KayÄ±t", f"{len(df_all):,}")
        st.metric("YÄ±l AralÄ±ÄŸÄ±", f"{df_all['year'].min()}-{df_all['year'].max()}")
        st.metric("Dosya SayÄ±sÄ±", df_all['source_file'].nunique())
        st.metric("Benzersiz Metrik", df_all['metric'].nunique())
    else:
        st.warning("Veri yÃ¼klenemedi")
    
    st.divider()
    st.header("ğŸ§ª RAGAS DeÄŸerlendirme")
    
    if st.button("â–¶ï¸ RAGAS Testi Ã‡alÄ±ÅŸtÄ±r", type="primary"):
        with st.spinner("Test Ã§alÄ±ÅŸÄ±yor..."):
            test_data = create_evaluation_dataset()
            results = evaluate_rag_with_ragas(rag_chain, retriever, test_data)
            
            if results:
                st.success("âœ… RAGAS deÄŸerlendirmesi tamamlandÄ±!")
                
                st.subheader("ğŸ“ˆ Metrik SonuÃ§larÄ±")
                
                metrics_df = pd.DataFrame([results])
                st.dataframe(metrics_df)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if 'answer_relevancy' in results:
                        st.metric("Answer Relevancy", f"{results['answer_relevancy']:.3f}")
                    if 'faithfulness' in results:
                        st.metric("Faithfulness", f"{results['faithfulness']:.3f}")
                
                with col2:
                    if 'context_recall' in results:
                        st.metric("Context Recall", f"{results['context_recall']:.3f}")
                    if 'context_precision' in results:
                        st.metric("Context Precision", f"{results['context_precision']:.3f}")
    
    st.divider()
    
    if st.button("ğŸ—‘ï¸ Vector DB SÄ±fÄ±rla", type="secondary"):
        import shutil
        if os.path.exists(PERSIST_DIR):
            shutil.rmtree(PERSIST_DIR)
            st.success("âœ… Silindi! SayfayÄ± yenileyin.")
            st.rerun()

# Chat Interface
if "messages" not in st.session_state:
    st.session_state.messages = []

if len(st.session_state.messages) == 0:
    st.info("ğŸ’¡ **Ã–rnek sorular:**")
    cols = st.columns(3)
    examples = [
        "2020 yÄ±lÄ± okullaÅŸma oranÄ± nedir?",
        "Ä°lkokul ve ortaokul karÅŸÄ±laÅŸtÄ±r",
        "Son yÄ±llarda Ã¶ÄŸretmen sayÄ±sÄ±"
    ]
    for col, q in zip(cols, examples):
        if col.button(q, key=f"ex_{q}"):
            st.session_state.messages.append({"role": "user", "content": q})
            st.rerun()

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

if user_query := st.chat_input("Sorunuzu yazÄ±n..."):
    st.session_state.messages.append({"role": "user", "content": user_query})
    
    with st.chat_message("user"):
        st.write(user_query)
    
    with st.chat_message("assistant"):
        with st.spinner("YanÄ±t hazÄ±rlanÄ±yor..."):
            try:
                lookup_text, matched = df_lookup_answer(df_all, user_query)

                if matched:
                    answer = lookup_text
                else:
                    docs = retriever.invoke(user_query)

                    if debug_mode:
                        with st.expander("ğŸ” Retrieved Contexts"):
                            for i, doc in enumerate(docs, 1):
                                st.write(f"**Doc {i}:**")
                                st.code(doc.page_content)
                                st.json(doc.metadata)

                    if not docs:
                        answer = "âŒ Bu soru iÃ§in veri bulamadÄ±m. LÃ¼tfen sorunuzu farklÄ± ÅŸekilde ifade edin."
                    else:
                        answer = rag_chain.invoke(user_query)

                st.write(answer)
                st.session_state.messages.append(
                    {"role": "assistant", "content": answer}
                )

            except Exception as e:
                st.error(f"âŒ Hata: {str(e)}")
                if debug_mode:
                    st.exception(e)

st.divider()
st.caption("ğŸ”’ TÃœÄ°K verileri | OpenAI: gpt-4o-mini + text-embedding-3-small")

