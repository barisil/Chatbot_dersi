import os
import re
import streamlit as st
from dotenv import load_dotenv
from pathlib import Path
from typing import List, Dict

from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ============================================
# YAPILANDIRMA

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    st.error("âš ï¸ LÃ¼tfen .env dosyasÄ±na OPENAI_API_KEY ekleyin.")
    st.stop()

# Sabitler
BASE_DIR = Path(__file__).resolve().parent
DATA_FOLDER = str(BASE_DIR / "data")
PERSIST_DIR = str(BASE_DIR / "chroma_db")

#Chatbotun en Ã¶nemli kÄ±sÄ±mlarÄ±ndan bir tanesi. Ä°deal deÄŸerler deÄŸiÅŸebiliyor ama 
#benim dokÃ¼manlardaki text ler iÃ§in genelde 500-800 arasÄ± chunk size ve %20 overlap iyi sonuÃ§ veriyor.


CHUNK_SIZE = 600
CHUNK_OVERLAP = 120
TOP_K = 4

# Streamlit yapÄ±landÄ±rma
st.set_page_config(
    page_title="TÃœÄ°K Ä°statistik Chatbot",
    page_icon="ğŸ“Š",
    layout="wide"
)

# LLM ve Embeddings GPT iÃ§in
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=1000, api_key=api_key)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small", api_key=api_key)

# Session state baÅŸlatma 
# Normalde streamlit her etkileÅŸimde sayfayÄ± tekrar baÅŸlatÄ±r, bunu yapmamasÄ± iÃ§in session_state kullanÄ±lÄ±r.
for key, default in [
    ("vector_store", None),
    ("retriever", None),
    ("messages", []),
    ("contexts", {})
]:
    if key not in st.session_state:
        st.session_state[key] = default

# Ground truth Ã¶lÃ§Ã¼mlemesi yapabilmek iÃ§in kullandÄ±ÄŸÄ±mÄ±z test verileri
GROUND_TRUTH = {
    "2020 yÄ±lÄ±nda genÃ§ nÃ¼fus oranÄ± nedir?": "2020 yÄ±lÄ±nda genÃ§ nÃ¼fus, toplam nÃ¼fusun %15,4'Ã¼nÃ¼ oluÅŸturdu.",
    "2023 yÄ±lÄ±nda akraba evliliÄŸi oranÄ± nedir?": "2023 yÄ±lÄ±nda akraba evliliÄŸi yapanlarÄ±n oranÄ± %8,2 oldu",
    "2014 yÄ±lÄ±nda boÅŸanan Ã§ift sayÄ±sÄ± kaÃ§tÄ±r?": "BoÅŸanan Ã§ift sayÄ±sÄ± 2014 yÄ±lÄ±nda 130 bin 913 oldu",
    "2018 yÄ±lÄ±nda genÃ§lerde iÅŸsizlik oranÄ± nedir?": "2018 yÄ±lÄ±nda genÃ§lerde iÅŸsizlik oranÄ± %20,3 oldu",
    "2020 yÄ±lÄ±nda ne eÄŸitimde ne istihdamda olan genÃ§lerin oranÄ± nedir?": "2020 yÄ±lÄ±nda ne eÄŸitimde ne istihdamda olan genÃ§lerin oranÄ± %28,3 oldu",
    "2023 yÄ±lÄ±nda internet kullanan genÃ§lerin oranÄ± nedir?": "2023 yÄ±lÄ±nda internet kullanan genÃ§lerin oranÄ± %97,5 oldu",
    "2024 yÄ±lÄ±nda yaÅŸlÄ± nÃ¼fus kaÃ§ kiÅŸidir?": "2024 yÄ±lÄ±nda yaÅŸlÄ± nÃ¼fus 9 milyon 112 bin 298 kiÅŸi oldu"
}

# KullanÄ±cÄ± sorusuyla eÅŸleÅŸtirebilmek iÃ§in normalize ediyoruz.
GROUND_TRUTH_NORM = {
    re.sub(r"\s+", " ", k.strip().lower()): v 
    for k, v in GROUND_TRUTH.items()
}

# ============================================
# YARDIMCI FONKSÄ°YONLAR

# retrieval ve RAG iÃ§in gerekli bir adÄ±m. Dosyalardaki kategori ve yÄ±l bilgilerini Ã§Ä±karÄ±r. SonrasÄ±nda bu bilgileri chunklara atayacaÄŸÄ±z.
# Verileri TÃœÄ°K ten yÄ±l bazlÄ± Ã§ektiÄŸim iÃ§in bu sÄ±nÄ±flandÄ±rma iÅŸe yarÄ±yor.
def extract_metadata(filename: str) -> Dict[str, str]:
    """Dosya adÄ±ndan kategori ve yÄ±l Ã§Ä±kar."""
    metadata = {"kategori": "bilinmiyor", "yil": "bilinmiyor"}
    
    filename_lower = filename.lower()
    if "genclik" in filename_lower:
        metadata["kategori"] = "genclik"
    elif "yasli" in filename_lower:
        metadata["kategori"] = "yasli"
    elif "aile" in filename_lower:
        metadata["kategori"] = "aile"
    
    year_match = re.search(r'_(\d{2})\.pdf', filename)
    if year_match:
        metadata["yil"] = f"20{year_match.group(1)}"
    
    return metadata

# Belirtilen klasÃ¶rdeki tÃ¼m PDF dosyalarÄ±nÄ± sayfa bazÄ±nda yÃ¼kler ve her sayfaya yÄ±l/kategori metadataâ€™sÄ± ekler.
# Bu fonksiyon sayesinde her chunk, hangi yÄ±l ve kategoriye ait olduÄŸunu biliyor
def load_pdfs(data_folder: str) -> List[Document]:
    if not os.path.exists(data_folder):
        st.error(f"âŒ {data_folder} klasÃ¶rÃ¼ bulunamadÄ±!")
        return []
    
    pdf_files = [f for f in os.listdir(data_folder) if f.lower().endswith('.pdf')]
    if not pdf_files:
        st.warning(f"âš ï¸ {data_folder} klasÃ¶rÃ¼nde PDF bulunamadÄ±!")
        return []
    
    all_documents = []
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, pdf_file in enumerate(pdf_files):
        status_text.text(f"YÃ¼kleniyor: {pdf_file}")
        
        try:
            loader = PyPDFLoader(os.path.join(data_folder, pdf_file))
            documents = loader.load()
            file_metadata = extract_metadata(pdf_file)
            
            for doc in documents:
                doc.metadata.update({
                    "source": pdf_file,
                    **file_metadata
                })
            
            all_documents.extend(documents)
        except Exception as e:
            st.warning(f"âš ï¸ {pdf_file} yÃ¼klenemedi: {str(e)}")
        
        progress_bar.progress((idx + 1) / len(pdf_files))
    
    progress_bar.empty()
    status_text.empty()
    
    return all_documents

# YÃ¼klenen dokÃ¼manlarÄ± parÃ§alara ayÄ±rÄ±r, her parÃ§anÄ±n embeddingâ€™ini Ã¼retir ve kalÄ±cÄ± bir vektÃ¶r veritabanÄ± oluÅŸturur.
def create_vector_store(documents: List[Document]) -> Chroma:
    """VektÃ¶r veritabanÄ± oluÅŸtur."""
    with st.spinner("ğŸ“ DokÃ¼manlar parÃ§alanÄ±yor..."):
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
        splits = splitter.split_documents(documents)
        st.info(f"âœ‚ï¸ Toplam {len(splits)} metin parÃ§asÄ± oluÅŸturuldu")
# Her chunk iÃ§in vektÃ¶r oluÅŸturuyoruz. VektÃ¶r veritabanÄ±nÄ± diske kalÄ±cÄ± olarak yazÄ±yoruz, bÃ¶ylece her Ã§alÄ±ÅŸtÄ±rmada embedding Ã¼retmiyoruz.
    with st.spinner("ğŸ”¢ Embeddings hesaplanÄ±yor..."):
        return Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory=PERSIST_DIR
        )

def extract_years(text: str) -> List[str]:
    """Metinden yÄ±l Ã§Ä±kar."""
    years = re.findall(r"\b(20\d{2})\b", text)
    return list(dict.fromkeys(years))  # SÄ±rayÄ± koruyarak unique yap


# Bu fonksiyon Ã¶ncesi karÅŸÄ±laÅŸtÄ±rma sorularÄ±nda bir yÄ±la Ã¶ncelik veriyordu. Åimdi tÃ¼m yÄ±llar iÃ§in farklÄ± aramalar yapÄ±yor ve sonuÃ§larÄ± birleÅŸtiriyor.
def retrieve_docs(question: str) -> List[Document]:
    """AkÄ±llÄ± dÃ¶kÃ¼man retrieval."""
    years = extract_years(question)
    # eÄŸer karÅŸÄ±laÅŸtÄ±rma yapÄ±lacaksa daha fazla sonuÃ§ almamÄ±z iÃ§in k'yÄ± artÄ±rÄ±yoruz
    k = 12 if len(years) >= 2 else TOP_K
    
    docs_all = []
    
    # YÄ±l bazlÄ± filtreleme
    if years and st.session_state.vector_store:
        for year in years:
            retriever = st.session_state.vector_store.as_retriever(
                search_kwargs={"k": k, "filter": {"yil": year}}
            )
            docs_all.extend(retriever.invoke(question))
        
        # az sonuÃ§ varsa genel aramaya dÃ¶nÃ¼yoruz
        if len(docs_all) < min(4, k):
            fallback = st.session_state.vector_store.as_retriever(
                search_kwargs={"k": k}
            )
            docs_all.extend(fallback.invoke(question))
    else:
        docs_all = st.session_state.retriever.invoke(question)
    
    # TekrarlarÄ± kaldÄ±r, context temizliÄŸi.
    unique_docs = []
    seen = set()
    for doc in docs_all:
        key = (
            doc.metadata.get("source"),
            doc.metadata.get("page"),
            doc.metadata.get("kategori"),
            doc.metadata.get("yil"),
            doc.page_content[:120]
        )
        if key not in seen:
            seen.add(key)
            unique_docs.append(doc)
    
    return unique_docs

# Soru ile getirilen baÄŸlam arasÄ±nda kavramsal uyumsuzluk varsa, cevap Ã¼retilmesini engeller. 
# YaÅŸanan sorunlardan sonra ekledim. Daha kapsamlÄ± arama sonucunda case sayÄ±sÄ± arttÄ±rÄ±labilir.

def guard_mismatch(question: str, docs: List[Document]) -> bool:
    q = question.lower()
    ctx = " ".join(d.page_content for d in docs).lower()
    
    # YaÅŸlÄ± nÃ¼fus oranÄ± vs yaÅŸlÄ± baÄŸÄ±mlÄ±lÄ±k oranÄ±
    if "yaÅŸlÄ± nÃ¼fus oran" in q and "yaÅŸlÄ± baÄŸÄ±mlÄ±lÄ±k oran" in ctx and "yaÅŸlÄ± nÃ¼fus oran" not in ctx:
        return True
    
    # Beklenen yaÅŸam sÃ¼resi tÃ¼rleri
    if "beklenen yaÅŸam sÃ¼resi" in q:
        if "doÄŸuÅŸta" in q and ("65 yaÅŸ" in ctx or "65 yaÅŸÄ±nda" in ctx) and "doÄŸuÅŸta" not in ctx:
            return True
        if ("65 yaÅŸ" in q or "65 yaÅŸÄ±nda" in q) and "doÄŸuÅŸta" in ctx and "65 yaÅŸ" not in ctx and "65 yaÅŸÄ±nda" not in ctx:
            return True
    
    return False

# Metadataâ€™yÄ± veritabanÄ±nda sÃ¼tun olarak tutuyoruz; format_docs ise bu sÃ¼tunlarÄ± raporda baÅŸlÄ±k olarak gÃ¶steriyor.
# Metadata tek kez Ã¼retiliyor; sistem boyunca taÅŸÄ±nÄ±yor; LLMâ€™e sadece okunur biÃ§imde sunuluyor.
# Retriever doÄŸru yÄ±l chunkâ€™Ä±nÄ± getirir ama LLM bu bilginin hangi yÄ±la ait olduÄŸunu anlamayabilir, farklÄ± chunkâ€™larÄ± karÄ±ÅŸtÄ±rabilir.
def format_docs(docs: List[Document]) -> str:
    """DÃ¶kÃ¼manlarÄ± formatla."""
    formatted = []
    for i, doc in enumerate(docs, 1):
        kategori = doc.metadata.get('kategori', 'bilinmiyor').upper()
        yil = doc.metadata.get('yil', 'bilinmiyor')
        formatted.append(f"[Kaynak {i} - {kategori} {yil}]\n{doc.page_content}\n")
    return "\n".join(formatted)

# Diskte kayÄ±tlÄ± bir vektÃ¶r veritabanÄ± varsa onu yÃ¼kler; yoksa PDFâ€™lerden sÄ±fÄ±rdan oluÅŸturur.
def init_vector_store():
    """VektÃ¶r veritabanÄ±nÄ± baÅŸlat veya yÃ¼kle."""
    if os.path.exists(PERSIST_DIR) and os.listdir(PERSIST_DIR):
        return Chroma(persist_directory=PERSIST_DIR, embedding_function=embeddings)
    
    documents = load_pdfs(DATA_FOLDER)
    if not documents:
        st.error(f"Veri bulunamadÄ±: {DATA_FOLDER}")
        return None
    
    return create_vector_store(documents)

def create_rag_chain():
    """RAG chain oluÅŸtur."""
    template = """Sen TÃœÄ°K istatistik uzmanÄ±sÄ±n. Soruyu DOÄRUDAN ve KISACA cevapla.

BaÄŸlam:
{context}

Soru:
{question}

CEVAP KURALLARI:
1. Soruyu DOÄRUDAN cevapla - gereksiz aÃ§Ä±klama yapma
2. SADECE sorulan bilgiyi ver - ek detay ekleme
3. SayÄ±sal bilgi varsa: "2023 yÄ±lÄ±nda oran %15,4'tÃ¼r." formatÄ±nda ver
4. KarÅŸÄ±laÅŸtÄ±rma isteniyorsa: KÄ±sa tablo veya liste kullan
5. "BaÄŸlama gÃ¶re...", "Kaynaklara gÃ¶re..." gibi giriÅŸler kullanma
6. Kavram uyumsuzluÄŸu varsa: "Bu bilgi dokÃ¼manlarda bulunmamaktadÄ±r."
7. **KRÄ°TÄ°K**: "Hangi yÄ±l", "en fazla", "en az" sorularÄ±nda:
   - SADECE baÄŸlamda verilen yÄ±llarÄ± karÅŸÄ±laÅŸtÄ±r
   - BaÄŸlamda olmayan yÄ±l veya veri ASLA ekleme
   - TÃ¼m yÄ±llarÄ±n verisi yoksa: "Mevcut verilere gÃ¶re [yÄ±l] yÄ±lÄ±nda [deÄŸer], ancak tÃ¼m yÄ±llarÄ±n verisi bulunmamaktadÄ±r."


YASAKLAR:
âŒ "Tabii ki", "Elbette", "Maalesef" gibi dolgu kelimeler
âŒ BaÄŸlamda olmayan genel bilgiler
âŒ "DÃ¼nya", "Avrupa", "OECD" karÅŸÄ±laÅŸtÄ±rmalarÄ± (baÄŸlamda yoksa)
âŒ Kavram karÄ±ÅŸtÄ±rma: "yaÅŸlÄ± nÃ¼fus oranÄ±" â‰  "yaÅŸlÄ± baÄŸÄ±mlÄ±lÄ±k oranÄ±"

Ã–rnek Ä°yi Cevap:
Soru: "2020'de genÃ§ iÅŸsizlik oranÄ± nedir?"
Cevap: "2020 yÄ±lÄ±nda genÃ§ iÅŸsizlik oranÄ± %25,9'dur."
Ã–rnek Ä°yi Cevap (KarÅŸÄ±laÅŸtÄ±rma):
Soru: "Hangi yÄ±lda genÃ§ iÅŸsizlik en yÃ¼ksekti?"
Cevap: "Mevcut verilere gÃ¶re 2018 yÄ±lÄ±nda genÃ§ iÅŸsizlik oranÄ± %20,3 ile en yÃ¼ksek seviyededir."


CEVAP (sadece cevap, baÅŸka hiÃ§bir ÅŸey yazma):"""
    prompt = ChatPromptTemplate.from_template(template)
    return prompt | llm | StrOutputParser()

# ============================================
# BAÅLATMA

# Retriever, vektÃ¶r veritabanÄ±na baÄŸÄ±mlÄ±. Bu yÃ¼zden Ã¶nce DBâ€™nin varlÄ±ÄŸÄ±nÄ± garanti altÄ±na alÄ±p, sonra retrieverâ€™Ä± baÅŸlatÄ±yorum.
if st.session_state.vector_store is None:
    st.session_state.vector_store = init_vector_store()

if st.session_state.vector_store and st.session_state.retriever is None:
    st.session_state.retriever = st.session_state.vector_store.as_retriever(
        search_kwargs={"k": TOP_K}
    )

# ============================================
# ANA UYGULAMA

#Bu bÃ¶lÃ¼mde chatbotun ana akÄ±ÅŸÄ± var. EÄŸer vektÃ¶r veritabanÄ± hazÄ±rsa sohbet aÃ§Ä±lÄ±yor. 
#Her kullanÄ±cÄ± sorusu iÃ§in Ã¶nce akÄ±llÄ± retrieval yapÄ±lÄ±yor, ardÄ±ndan kavram uyumsuzluÄŸu kontrol ediliyor. 
#GÃ¼venliyse cevap Ã¼retiliyor; deÄŸilse â€˜bilgi yokâ€™ deniyor. AynÄ± anda tÃ¼m soru cevap context verisi RAGAS deÄŸerlendirmesi iÃ§in loglanÄ±yor."""

st.title("ğŸ“Š TÃ¼rkiye GenÃ§lik, Aile ve YaÅŸlÄ± Ä°statistikleri Chatbot")
st.caption("OpenAI GPT + RAGAS ile performans deÄŸerlendirmeli versiyon")

if st.session_state.retriever:
    rag_chain = create_rag_chain()
    
    st.subheader("ğŸ’¬ Sohbet")
    
    # Mesaj geÃ§miÅŸi
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            
            if message["role"] == "assistant" and "sources" in message:
                with st.expander("ğŸ“š Kaynaklar"):
                    for i, source in enumerate(message["sources"], 1):
                        st.markdown(f"**Kaynak {i}:** {source['source']}")
                        st.markdown(f"*Kategori:* {source['kategori']} | *YÄ±l:* {source['yil']}")
                        st.text(source['content'][:200] + "...")
                        st.divider()
    
    # KullanÄ±cÄ± giriÅŸi
    if prompt := st.chat_input("Sorunuzu sorun..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("DÃ¼ÅŸÃ¼nÃ¼yorum..."):
                retrieved_docs = retrieve_docs(prompt)
                
                if not retrieved_docs:
                    response = "Bu bilgi verilen dokÃ¼manlarda bulunmamaktadÄ±r."
                    contexts = []
                elif guard_mismatch(prompt, retrieved_docs):
                    response = "Bu bilgi verilen dokÃ¼manlarda bulunmamaktadÄ±r."
                    contexts = [doc.page_content for doc in retrieved_docs]
                else:
                    context_text = format_docs(retrieved_docs)
                    response = rag_chain.invoke({"context": context_text, "question": prompt})
                    contexts = [doc.page_content for doc in retrieved_docs]
                
                # Ground truth ve context kaydet
                norm_question = re.sub(r"\s+", " ", prompt.strip().lower())
                st.session_state.contexts[prompt] = {
                    "question": prompt,
                    "answer": response,
                    "contexts": contexts,
                    "ground_truth": GROUND_TRUTH_NORM.get(norm_question, "")
                }
                
                st.markdown(response)
                
                # KaynaklarÄ± gÃ¶ster
                sources = [
                    {
                        "source": doc.metadata.get('source', 'Bilinmiyor'),
                        "kategori": doc.metadata.get('kategori', '-'),
                        "yil": doc.metadata.get('yil', '-'),
                        "content": doc.page_content
                    }
                    for doc in retrieved_docs
                ]
                
                with st.expander("ğŸ“š Kaynaklar"):
                    for i, source in enumerate(sources, 1):
                        st.markdown(f"**Kaynak {i}:** {source['source']}")
                        st.markdown(f"*Kategori:* {source['kategori']} | *YÄ±l:* {source['yil']}")
                        st.text(source['content'][:200] + "...")
                        st.divider()
        
        st.session_state.messages.append({
            "role": "assistant",
            "content": response,
            "sources": sources
        })

else:
    st.info("ğŸ‘ˆ LÃ¼tfen sol menÃ¼den PDF'leri iÅŸleyip veritabanÄ±nÄ± oluÅŸturun.")
    
    st.markdown("### ğŸ“‹ Ã–rnek Sorular")
    st.markdown("""
    - 2020 yÄ±lÄ±nda genÃ§lerin iÅŸsizlik oranÄ± nedir?
    - 2014 ile 2024 arasÄ±nda aile yapÄ±sÄ± nasÄ±l deÄŸiÅŸti?
    - YaÅŸlÄ± nÃ¼fus oranÄ± yÄ±llara gÃ¶re nasÄ±l bir trend gÃ¶steriyor?
    - En son yÄ±l iÃ§in genÃ§lik istatistikleri nedir?
    - Hangi yÄ±llarda evlilik oranÄ± en yÃ¼ksekti?
    """)
    
    st.markdown("### ğŸ“Š Sistem Ã–zellikleri")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Model", "GPT-4o Mini")
        st.metric("Embedding", "text-embedding-3-small")
    
    with col2:
        st.metric("Chunk Size", CHUNK_SIZE)
        st.metric("Chunk Overlap", CHUNK_OVERLAP)
    
    with col3:
        st.metric("Top-K", TOP_K)
        st.metric("Temperature", 0)

# ============================================
# ALT BÄ°LGÄ°
# ============================================

st.divider()
col1, col2, col3 = st.columns(3)

with col1:
    if st.session_state.messages:
        if st.button("ğŸ§¹ Sohbeti Temizle"):
            st.session_state.messages = []
            st.session_state.contexts = {}
            st.rerun()

with col2:
    if st.session_state.contexts:
        st.info(f"âœ… {len(st.session_state.contexts)} soru RAGAS iÃ§in hazÄ±r")

with col3:
    if st.button("ğŸ“ˆ RAGAS DeÄŸerlendirmesine Git"):
        st.switch_page("pages/ragas_evaluation.py")