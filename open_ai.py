import os
import re
import streamlit as st
from dotenv import load_dotenv

from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

import pandas as pd

# ---------------------------
# Config
# ---------------------------
load_dotenv()
openai_key = os.getenv("OPENAI_API_KEY")

if not openai_key:
    st.error("âš ï¸ LÃ¼tfen .env dosyasÄ±na OPENAI_API_KEY ekleyin.")
    st.stop()

llm = ChatOpenAI(
    model="gpt-4o-mini",  # Ekonomik ve hÄ±zlÄ±
    temperature=0.3,
    max_tokens=1000,
)

st.set_page_config(page_title="Ã‡ocuk Okul EÄŸitimi Chatbot", layout="wide")
st.title("ğŸ“š Ã‡ocuk EÄŸitimi Ä°statistikleri Chatbot")
st.caption("TÃœÄ°K verilerine dayalÄ± eÄŸitim istatistikleri analiz sistemi (OpenAI)")

DATA_FOLDER = "data"
PERSIST_DIR = "./chroma_db_openai"


# ---------------------------
# Helpers
# ---------------------------
def parse_tuik_pipe_rows(file_path: str, encoding="utf-8"):
    """TÃœÄ°K pipe-delimited formatÄ±nÄ± parse eder"""
    rows = []
    current_metric = None
    current_breakdown = None
    current_geo = None

    def is_year(s: str) -> bool:
        return s.isdigit() and 1900 <= int(s) <= 2100

    def parse_value(s: str):
        s = str(s).strip().replace(",", ".")
        try:
            return float(s)
        except:
            return None

    try:
        with open(file_path, "r", encoding=encoding, errors="replace") as f:
            for raw in f:
                line = raw.strip()
                if not line:
                    continue
                if "SÃ¼tunlar" in line or line.startswith("SatÄ±rlar"):
                    continue

                parts = [p.strip() for p in line.split("|")]
                while parts and parts[0] == "":
                    parts.pop(0)
                while parts and parts[-1] == "":
                    parts.pop()

                if not parts:
                    continue

                if len(parts) == 1 and re.match(r".+-[A-Z]{2}$", parts[0]):
                    current_geo = parts[0]
                    continue

                if len(parts) >= 4 and is_year(parts[-2]):
                    metric, breakdown, year, value = parts[-4], parts[-3], parts[-2], parts[-1]
                    if metric:
                        current_metric = metric
                    if breakdown:
                        current_breakdown = breakdown

                    rows.append({
                        "metric": current_metric,
                        "breakdown": current_breakdown,
                        "year": int(year),
                        "value": parse_value(value),
                        "geo": current_geo
                    })
                    continue

                if len(parts) == 3 and is_year(parts[1]):
                    breakdown, year, value = parts
                    if breakdown:
                        current_breakdown = breakdown
                    rows.append({
                        "metric": current_metric,
                        "breakdown": current_breakdown,
                        "year": int(year),
                        "value": parse_value(value),
                        "geo": current_geo
                    })
                    continue

                if len(parts) == 2 and is_year(parts[0]):
                    year, value = parts
                    rows.append({
                        "metric": current_metric,
                        "breakdown": current_breakdown,
                        "year": int(year),
                        "value": parse_value(value),
                        "geo": current_geo
                    })
                    continue

                if len(parts) == 2:
                    m, b = parts
                    if m:
                        current_metric = m
                    if b:
                        current_breakdown = b

        return [r for r in rows if r["year"] and r["value"] is not None]
    
    except Exception as e:
        st.warning(f"âš ï¸ {file_path} dosyasÄ± okunurken hata: {str(e)}")
        return []


def build_docs_for_one_csv(file_path: str):
    """Bir CSV dosyasÄ±ndan Document nesneleri oluÅŸturur"""
    rows = parse_tuik_pipe_rows(file_path)
    if not rows:
        return []

    dataset_name = os.path.basename(file_path).replace(".csv", "")
    years = [r["year"] for r in rows]
    min_year, max_year = min(years), max(years)

    metrics = sorted({r["metric"] for r in rows if r["metric"]})[:5]
    breakdowns = sorted({r["breakdown"] for r in rows if r["breakdown"]})[:5]

    docs = []

    # Dataset Ã¶zet
    desc = (
        f"Veri Seti: {dataset_name}\n"
        f"Kapsam: TÃœÄ°K Ã§ocuk eÄŸitimi istatistikleri\n"
        f"YÄ±l AralÄ±ÄŸÄ±: {min_year}-{max_year}\n"
        f"Ana Metrikler: {', '.join(metrics)}\n"
        f"KÄ±rÄ±lÄ±mlar: {', '.join(breakdowns)}"
    )

    docs.append(Document(
        page_content=desc,
        metadata={
            "type": "dataset_summary",
            "dataset": dataset_name,
            "year_range": f"{min_year}-{max_year}"
        }
    ))

    # YÄ±llÄ±k Ã¶zetler
    df_temp = pd.DataFrame(rows)
    for year in sorted(df_temp["year"].unique())[-5:]:  # Son 5 yÄ±l
        year_data = df_temp[df_temp["year"] == year]
        summary = (
            f"{year} YÄ±lÄ± Ã–zeti - {dataset_name}:\n"
            f"Toplam veri noktasÄ±: {len(year_data)}\n"
            f"Ortalama deÄŸer: {year_data['value'].mean():.2f}\n"
            f"Min: {year_data['value'].min()}, Max: {year_data['value'].max()}"
        )
        docs.append(Document(
            page_content=summary,
            metadata={
                "type": "year_summary",
                "dataset": dataset_name,
                "year": year
            }
        ))

    # DetaylÄ± veri noktalarÄ±
    for r in rows[:80]:
        geo_text = r['geo'] if r['geo'] else 'TÃ¼rkiye'
        
        content = (
            f"{geo_text}, {r['year']}: {r['metric']} - {r['breakdown']} = {r['value']}"
        )
        
        docs.append(Document(
            page_content=content,
            metadata={
                "type": "data_point",
                "dataset": dataset_name,
                "year": r["year"],
                "metric": r["metric"],
                "geo": geo_text
            }
        ))

    return docs


def format_docs(docs):
    if not docs:
        return "Ä°lgili veri bulunamadÄ±."
    return "\n\n".join(d.page_content for d in docs)


@st.cache_resource
def load_all_rows_as_df():
    """TÃ¼m CSV dosyalarÄ±nÄ± DataFrame olarak yÃ¼kler"""
    all_rows = []
    
    if not os.path.exists(DATA_FOLDER):
        st.error(f"âŒ {DATA_FOLDER} klasÃ¶rÃ¼ bulunamadÄ±!")
        return pd.DataFrame(columns=["metric","breakdown","year","value","geo","source_file"])
    
    csv_files = [f for f in os.listdir(DATA_FOLDER) if f.endswith(".csv")]
    
    if not csv_files:
        st.warning(f"âš ï¸ {DATA_FOLDER} klasÃ¶rÃ¼nde CSV dosyasÄ± bulunamadÄ±!")
        return pd.DataFrame(columns=["metric","breakdown","year","value","geo","source_file"])
    
    for fn in csv_files:
        fp = os.path.join(DATA_FOLDER, fn)
        rows = parse_tuik_pipe_rows(fp)
        for r in rows:
            r["source_file"] = fn
        all_rows.extend(rows)

    if not all_rows:
        return pd.DataFrame(columns=["metric","breakdown","year","value","geo","source_file"])

    df = pd.DataFrame(all_rows)
    df["metric"] = df["metric"].fillna("").astype(str)
    df["breakdown"] = df["breakdown"].fillna("").astype(str)
    df["geo"] = df["geo"].fillna("TÃ¼rkiye-TR").astype(str)
    
    return df


@st.cache_resource
def prepare_vector_db():
    """Vector database'i hazÄ±rlar - OpenAI embeddings"""
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small"  # Ekonomik embedding modeli
    )

    if os.path.exists(PERSIST_DIR) and os.listdir(PERSIST_DIR):
        st.info("â™»ï¸ Mevcut vector DB yÃ¼kleniyor...")
        return Chroma(
            persist_directory=PERSIST_DIR,
            embedding_function=embeddings
        )

    st.info("ğŸ”„ Vector DB oluÅŸturuluyor...")
    docs = []
    csv_files = [f for f in os.listdir(DATA_FOLDER) if f.endswith(".csv")]
    
    if not csv_files:
        raise ValueError(f"âŒ {DATA_FOLDER} klasÃ¶rÃ¼nde CSV dosyasÄ± bulunamadÄ±!")
    
    for fn in csv_files:
        fp = os.path.join(DATA_FOLDER, fn)
        file_docs = build_docs_for_one_csv(fp)
        docs.extend(file_docs)
        st.caption(f"âœ“ {fn}: {len(file_docs)} dokÃ¼man")

    if not docs:
        raise ValueError("âŒ HiÃ§ dokÃ¼man Ã¼retilemedi!")

    st.info(f"ğŸ“Š {len(docs)} dokÃ¼man indexleniyor...")
    
    return Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        persist_directory=PERSIST_DIR
    )


# ---------------------------
# RAG Chain Setup
# ---------------------------
try:
    vector_store = prepare_vector_db()
    st.success(f"âœ… Vector DB hazÄ±r ({PERSIST_DIR})")
except Exception as e:
    st.error(f"âŒ Vector DB hatasÄ±: {str(e)}")
    st.stop()

retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)

prompt = ChatPromptTemplate.from_template(
"""Sen Ã§ocuk eÄŸitimi istatistikleri uzmanÄ±sÄ±n. TÃœÄ°K verilerini kullanarak sorularÄ± yanÄ±tlÄ±yorsun.

BaÄŸlam:
{context}

Soru: {question}

Kurallar:
- Sadece verilen baÄŸlamdaki bilgileri kullan
- BaÄŸlamda bilgi yoksa "Bu konuda veri bulamadÄ±m" de
- SayÄ±larÄ± net belirt, kaynak gÃ¶ster
- KÄ±sa ve Ã¶z yanÄ±t ver

Cevap:"""
)

context_runnable = retriever | RunnableLambda(format_docs)

rag_chain = (
    {"context": context_runnable, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)


# ---------------------------
# Streamlit UI
# ---------------------------
df_all = load_all_rows_as_df()

# Sidebar
with st.sidebar:
    st.header("ğŸ“Š Veri Seti Bilgileri")
    
    if not df_all.empty:
        st.metric("Toplam KayÄ±t", len(df_all))
        st.metric("YÄ±l AralÄ±ÄŸÄ±", f"{df_all['year'].min()}-{df_all['year'].max()}")
        st.metric("FarklÄ± Metrik", df_all['metric'].nunique())
        st.metric("CSV DosyasÄ±", df_all['source_file'].nunique())
        
        with st.expander("ğŸ“ Dosyalar"):
            for file in df_all['source_file'].unique():
                st.write(f"â€¢ {file}")
    
    st.divider()
    debug_mode = st.checkbox("ğŸ”§ Debug Modu", value=False)
    
    if st.button("ğŸ—‘ï¸ Vector DB SÄ±fÄ±rla"):
        import shutil
        if os.path.exists(PERSIST_DIR):
            shutil.rmtree(PERSIST_DIR)
            st.success("Silindi. SayfayÄ± yenileyin.")
            st.rerun()

# Chat
if "messages" not in st.session_state:
    st.session_state.messages = []

if len(st.session_state.messages) == 0:
    st.info("ğŸ’¡ **Ã–rnek sorular:**")
    cols = st.columns(3)
    examples = [
        "2020 yÄ±lÄ±nda okullaÅŸma oranÄ± nedir?",
        "Ä°lkokul ve ortaokul karÅŸÄ±laÅŸtÄ±rmasÄ±",
        "Son 5 yÄ±lda Ã¶ÄŸretmen sayÄ±sÄ± deÄŸiÅŸimi"
    ]
    for col, q in zip(cols, examples):
        if col.button(q, key=f"ex_{q}"):
            st.session_state.messages.append({"role": "user", "content": q})
            st.rerun()

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

if user_query := st.chat_input("Sorunuzu yazÄ±n..."):
    st.session_state.messages.append({"role": "user", "content": user_query})
    
    with st.chat_message("user"):
        st.write(user_query)

    with st.chat_message("assistant"):
        with st.spinner("YanÄ±t hazÄ±rlanÄ±yor..."):
            try:
                docs = retriever.invoke(user_query)
                
                if debug_mode:
                    with st.expander("ğŸ” Retrieved Context"):
                        for i, doc in enumerate(docs, 1):
                            st.write(f"**Doc {i}:**")
                            st.write(doc.page_content)
                            st.json(doc.metadata)
                
                if len(docs) == 0:
                    answer = "Bu soru iÃ§in veri bulamadÄ±m. FarklÄ± bir soru deneyin."
                else:
                    answer = rag_chain.invoke(user_query)
                
                st.write(answer)
                st.session_state.messages.append(
                    {"role": "assistant", "content": answer}
                )
            
            except Exception as e:
                error_msg = f"âŒ Hata: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append(
                    {"role": "assistant", "content": error_msg}
                )

st.divider()
st.caption("ğŸ”’ TÃœÄ°K verilerine dayalÄ± | Powered by OpenAI GPT-4o-mini")